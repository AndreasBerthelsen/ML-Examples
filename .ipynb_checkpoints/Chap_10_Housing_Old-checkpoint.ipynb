{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 10 â€“ Introduction to Artificial Neural Networks with Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression using the California housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import sys\n",
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, split and scale the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.068558169089147"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the mean median_house_value.\n",
    "# We can see that the target value have been scaled down by a factor of 100000\n",
    "# compared to the Housing dataset used in chapter 2. We should keep this in mind,\n",
    "# if we want to compare the RMSE of this model with the RMSEs of the other models\n",
    "# that we trained and tested using the dataset from chapter 2.\n",
    "housing.target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, compile, train and evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 17:03:29.520190: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Build a model.\n",
    "# We don't need to specify an input layer, since we don't need to convert the input array.\n",
    "# For regression problems, we don't use an activation function in the output layer.\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # hidden layer\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    # output layer\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "# For regression problems, we use the \"mean_squared_error\" as loss function.\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 952us/step - loss: 0.7294 - val_loss: 15.9921\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.6180 - val_loss: 9.6009\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 735us/step - loss: 0.5292 - val_loss: 0.4534\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 799us/step - loss: 0.3957 - val_loss: 0.3639\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 757us/step - loss: 0.3849 - val_loss: 0.3605\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 772us/step - loss: 0.3776 - val_loss: 0.3825\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 781us/step - loss: 0.3765 - val_loss: 0.3767\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 767us/step - loss: 0.3689 - val_loss: 0.3864\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 795us/step - loss: 0.3649 - val_loss: 0.4068\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 789us/step - loss: 0.3633 - val_loss: 0.3810\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 811us/step - loss: 0.3598 - val_loss: 0.3596\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 767us/step - loss: 0.3585 - val_loss: 0.3756\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 794us/step - loss: 0.3549 - val_loss: 0.3618\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 778us/step - loss: 0.3520 - val_loss: 0.3504\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.3546 - val_loss: 0.3635\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 745us/step - loss: 0.3494 - val_loss: 0.3388\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 708us/step - loss: 0.3476 - val_loss: 0.3475\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 715us/step - loss: 0.3467 - val_loss: 0.3460\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 759us/step - loss: 0.3551 - val_loss: 0.3390\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.3451 - val_loss: 0.3897\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the learning curves.         \n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 543us/step - loss: 0.3437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34374409914016724"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model (outputs the RMSE).\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:</b><br/>\n",
    "If we multipy the RMSE (=loss) with 100000, we can compare with values obtained by the best models that I trained and tested on the dataset used in chapter 2.\n",
    "\n",
    "Gradient Boosted Forest: 47480 (best model that I trained and tested on the dataset used in chapter 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71222204],\n",
       "       [1.668096  ],\n",
       "       [4.1280465 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions for the first 3 instances in the test set.\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.477  , 0.458  , 5.00001])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with the corresponding values target values\n",
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:</b><br/>\n",
    "The second prediction is very bad given the RMSE. The other two predictions are okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "We can tune hyperparameters by using Scikit-Learn's <b>GridSearchCV</b> or <b>RandomizedSearchCV</b>. The first will train the network with every combination of the specified hyperparameters, while the latter will randomly pick a number of combinations. For this example, we will use RandomizedSearchCV to avoid getting too many combinations.\n",
    "\n",
    "We need to wrap the compiled Keras model in an object that mimic a regular Scikit-Learn regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a compiled Keras model and wrap it in a Scikit-Learn KerasRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will build and compile a Keras model.\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/qdqg9zqx2d9_tpxp95w6fg6r0000gn/T/ipykernel_24250/805132444.py:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "# Wrap the Keras model in a Scikit-Learn KerasRegressor.\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RandomizedSearchCV to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 17:03:47.537474: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.537505: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.537626: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.537741: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.538169: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.538171: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.538228: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.538237: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.538250: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.538518: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.538678: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 17:03:47.540750: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 3.2741 - val_loss: 5.2301\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 3.7047 - val_loss: 5.6051\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 3.3903 - val_loss: 12.1139\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 2.6874 - val_loss: 3.2952\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 2.4255 - val_loss: 5.2994\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 2.7179 - val_loss: 10.6041\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.9330 - val_loss: 0.8449\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 3s 7ms/step - loss: 0.9525 - val_loss: 5.4195\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 3s 7ms/step - loss: 1.3169 - val_loss: 1.8537\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 3s 7ms/step - loss: 1.3708 - val_loss: 5.1978\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 3s 7ms/step - loss: 1.3048 - val_loss: 0.5734\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 3s 7ms/step - loss: 1.2345 - val_loss: 2.3527\n",
      "118/242 [=============>................] - ETA: 0s - loss: 1.6926Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.4876 - val_loss: 1.8583\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.5743 - val_loss: 7.2636\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.6095 - val_loss: 4.3050\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.1697 - val_loss: 1.9671\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0139 - val_loss: 1.6213\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0544 - val_loss: 13.0035\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6326 - val_loss: 0.7167\n",
      " 79/242 [========>.....................] - ETA: 0s - loss: 1.1091Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6927 - val_loss: 10.3618\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4944 - val_loss: 0.4603\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6380 - val_loss: 1.0544\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5346 - val_loss: 3.8905\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5460 - val_loss: 0.5172\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9873 - val_loss: 1.0250\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.1173 - val_loss: 6.8002\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9785 - val_loss: 1.8982\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7786 - val_loss: 0.8254\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8068 - val_loss: 10.7781\n",
      " 78/242 [========>.....................] - ETA: 0s - loss: 0.9230Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8350 - val_loss: 0.9314\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5921 - val_loss: 2.8934\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5416 - val_loss: 0.5530\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5038 - val_loss: 0.5075\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4310 - val_loss: 0.4011\n",
      " 23/242 [=>............................] - ETA: 0s - loss: 0.5222Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4628 - val_loss: 0.4090\n",
      " 81/242 [=========>....................] - ETA: 0s - loss: 0.6724Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4966 - val_loss: 2.4337\n",
      " 33/242 [===>..........................] - ETA: 0s - loss: 0.5111Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8361 - val_loss: 0.7905\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9623 - val_loss: 5.5747\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7787 - val_loss: 0.9898\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7167 - val_loss: 0.6871\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7117 - val_loss: 0.6740\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7182 - val_loss: 8.5644\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4574 - val_loss: 0.4321\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4905 - val_loss: 0.4748\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4652 - val_loss: 0.4260\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3968 - val_loss: 0.5027\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4227 - val_loss: 0.5175\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4455 - val_loss: 0.4702\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7786 - val_loss: 0.7577\n",
      "219/242 [==========================>...] - ETA: 0s - loss: 0.4081Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8857 - val_loss: 4.3968\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7107 - val_loss: 0.7373\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6628 - val_loss: 0.6366\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6738 - val_loss: 6.7417\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4330 - val_loss: 0.4205\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4576 - val_loss: 0.4602\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6811 - val_loss: 0.6377\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4059 - val_loss: 0.4021\n",
      " 85/242 [=========>....................] - ETA: 0s - loss: 0.6778Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3809 - val_loss: 0.5694\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4008 - val_loss: 0.4038\n",
      " 20/242 [=>............................] - ETA: 0s - loss: 0.4428Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3953 - val_loss: 0.4040: 0.424\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7480 - val_loss: 0.7543 0s - loss: 0.661\n",
      "225/242 [==========================>...] - ETA: 0s - loss: 0.6287Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8377 - val_loss: 3.4004\n",
      "209/242 [========================>.....] - ETA: 0s - loss: 0.4155Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6816 - val_loss: 0.6635\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6313 - val_loss: 0.6098.741\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6453 - val_loss: 5.0056\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6589 - val_loss: 0.6151\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4352 - val_loss: 0.5214\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4171 - val_loss: 0.4813\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.4178Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3934 - val_loss: 0.3912\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3693 - val_loss: 0.7821\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3809 - val_loss: 1.0006\n",
      "205/242 [========================>.....] - ETA: 0s - loss: 0.7245Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3808 - val_loss: 0.4071\n",
      " 44/242 [====>.........................] - ETA: 0s - loss: 0.3527Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7264 - val_loss: 0.7308\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8022 - val_loss: 2.6046\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6635 - val_loss: 0.6414..............] - ETA: 0s - loss: 0.796\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6081 - val_loss: 0.5868\n",
      " 97/242 [===========>..................] - ETA: 0s - loss: 0.7962Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6400 - val_loss: 0.5970\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4074 - val_loss: 0.3740\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6226 - val_loss: 3.8047\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3855 - val_loss: 0.3785\n",
      "Epoch 8/100\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3583 - val_loss: 0.7703..............] - ETA: 0s - loss: 0.6193\n",
      " 43/242 [====>.........................] - ETA: 0s - loss: 0.6634Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4184 - val_loss: 0.5960\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3778 - val_loss: 1.3320\n",
      " 39/242 [===>..........................] - ETA: 0s - loss: 0.3367Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3729 - val_loss: 0.3825\n",
      " 71/242 [=======>......................] - ETA: 0s - loss: 0.3593Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7083 - val_loss: 0.6986\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7734 - val_loss: 1.9622\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6493 - val_loss: 0.6265\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5886 - val_loss: 0.5689\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6033 - val_loss: 2.9554\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6235 - val_loss: 0.5811\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3980 - val_loss: 0.4594\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4063 - val_loss: 0.6901\n",
      "Epoch 9/100\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3790 - val_loss: 0.3824\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3555 - val_loss: 0.7094\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3735 - val_loss: 0.7617\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3655 - val_loss: 0.3832\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6918 - val_loss: 0.6741\n",
      "221/242 [==========================>...] - ETA: 0s - loss: 0.6103Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7485 - val_loss: 1.4964\n",
      "224/242 [==========================>...] - ETA: 0s - loss: 0.3921Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6359 - val_loss: 0.6131\n",
      "204/242 [========================>.....] - ETA: 0s - loss: 0.3410Epoch 10/10038\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6083 - val_loss: 0.5655\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5722 - val_loss: 0.5487\n",
      "228/242 [===========================>..] - ETA: 0s - loss: 0.3622Epoch 10/100\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5860 - val_loss: 2.3536\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3959 - val_loss: 0.8112\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3922 - val_loss: 0.3668\n",
      " 63/242 [======>.......................] - ETA: 0s - loss: 0.5989Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3739 - val_loss: 0.3923\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3501 - val_loss: 1.0775\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3609 - val_loss: 0.3405\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3587 - val_loss: 0.3723\n",
      " 48/242 [====>.........................] - ETA: 0s - loss: 0.3699Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6768 - val_loss: 0.6518...] - ETA: 0s - loss: 0.367\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7264 - val_loss: 1.1490\n",
      "177/242 [====================>.........] - ETA: 0s - loss: 0.3399Epoch 11/100\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6238 - val_loss: 0.6003........] - ETA: 0s - loss: 0.353\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5570 - val_loss: 0.5330\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5942 - val_loss: 0.5522\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5703 - val_loss: 1.8946\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3892 - val_loss: 0.8608\n",
      "180/242 [=====================>........] - ETA: 0s - loss: 0.7009Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3694 - val_loss: 0.3753\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3466 - val_loss: 0.9566\n",
      "225/242 [==========================>...] - ETA: 0s - loss: 0.6639Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3856 - val_loss: 0.4158\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3533 - val_loss: 0.3457\n",
      "182/242 [=====================>........] - ETA: 0s - loss: 0.6084Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3545 - val_loss: 0.3626\n",
      " 75/242 [========>.....................] - ETA: 0s - loss: 0.3677Epoch 11/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6624 - val_loss: 0.6526\n",
      " 81/242 [=========>....................] - ETA: 0s - loss: 0.3685Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7064 - val_loss: 0.9036\n",
      " 22/242 [=>............................] - ETA: 1s - loss: 0.5725Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6121 - val_loss: 0.5884\n",
      "232/242 [===========================>..] - ETA: 0s - loss: 0.5847Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5435 - val_loss: 0.5196\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5560 - val_loss: 1.4892\n",
      "Epoch 12/100\n",
      "Epoch 12/100\n",
      "242/242 [====================>.........] - ETA: 0s - loss: 0.6432 [==============================] - 1s 4ms/step - loss: 0.5813 - val_loss: 0.5403\n",
      "  1/242 [..............................] - ETA: 1s - loss: 0.5502Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3843 - val_loss: 1.0620\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3659 - val_loss: 0.3838\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3814 - val_loss: 0.4150\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3431 - val_loss: 0.9729\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3483 - val_loss: 0.3674\n",
      " 72/242 [=======>......................] - ETA: 0s - loss: 0.3799Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6010 - val_loss: 0.5775\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6882 - val_loss: 0.7494\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3495 - val_loss: 0.3555\n",
      "Epoch 13/100\n",
      "Epoch 13/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.6532Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6492 - val_loss: 0.6410\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/242 [====================>.........] - ETA: 0s - loss: 0.5126\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "218/242 [==========================>...] - ETA: 0s - loss: 0.3703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 29/242 [==>...........................] - ETA: 0s - loss: 0.7202\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 29/242 [==>...........................] - ETA: 0s - loss: 0.6115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 29/242 [==>...........................] - ETA: 0s - loss: 0.3115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 32/242 [==>...........................] - ETA: 0s - loss: 0.6279\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 68/242 [=======>......................] - ETA: 0s - loss: 0.3335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "172/242 [====================>.........] - ETA: 0s - loss: 0.5459\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "144/242 [================>.............] - ETA: 0s - loss: 0.3820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "179/242 [=====================>........] - ETA: 0s - loss: 0.5165\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "184/242 [=====================>........] - ETA: 0s - loss: 0.5675\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "231/242 [===========================>..] - ETA: 0s - loss: 0.3673\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 43/242 [====>.........................] - ETA: 0s - loss: 0.6013\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 89/242 [==========>...................] - ETA: 0s - loss: 0.3346\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "166/242 [===================>..........] - ETA: 0s - loss: 0.3839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 47/242 [====>.........................] - ETA: 0s - loss: 0.3207\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "192/242 [======================>.......] - ETA: 0s - loss: 0.5452\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "204/242 [========================>.....] - ETA: 0s - loss: 0.5691\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "199/242 [=======================>......] - ETA: 0s - loss: 0.5256\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 45/242 [====>.........................] - ETA: 0s - loss: 0.6330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "196/242 [=======================>......] - ETA: 0s - loss: 0.3448\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 98/242 [===========>..................] - ETA: 0s - loss: 0.3374\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "208/242 [========================>.....] - ETA: 0s - loss: 0.5267\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 52/242 [=====>........................] - ETA: 0s - loss: 0.3188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 49/242 [=====>........................] - ETA: 0s - loss: 0.6329\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "201/242 [=======================>......] - ETA: 0s - loss: 0.5481\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "178/242 [=====================>........] - ETA: 0s - loss: 0.3855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "216/242 [=========================>....] - ETA: 0s - loss: 0.5720\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 71/242 [=======>......................] - ETA: 0s - loss: 0.6034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "213/242 [=========================>....] - ETA: 0s - loss: 0.3800\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 38/242 [===>..........................] - ETA: 0s - loss: 0.7041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "197/242 [=======================>......] - ETA: 0s - loss: 0.3455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "214/242 [=========================>....] - ETA: 0s - loss: 0.3799\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 39/242 [===>..........................] - ETA: 1s - loss: 0.7123\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 60/242 [======>.......................] - ETA: 0s - loss: 0.3305\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 55/242 [=====>........................] - ETA: 0s - loss: 0.6294\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 74/242 [========>.....................] - ETA: 0s - loss: 0.5994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "206/242 [========================>.....] - ETA: 0s - loss: 0.5465\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "104/242 [===========>..................] - ETA: 0s - loss: 0.3382\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3629 - val_loss: 0.3697\n",
      "148/242 [=================>............] - ETA: 0s - loss: 0.5912Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3770 - val_loss: 1.1513\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5310 - val_loss: 0.5065\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5692 - val_loss: 0.5278\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5429 - val_loss: 1.2436\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3800 - val_loss: 0.3792\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3419 - val_loss: 0.6415\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3482 - val_loss: 0.3618\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6715 - val_loss: 0.6569\n",
      "106/242 [============>.................] - ETA: 0s - loss: 0.5590Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6368 - val_loss: 0.6214\n",
      "Epoch 13/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.5572Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3476 - val_loss: 0.3540\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5903 - val_loss: 0.5731\n",
      "128/242 [==============>...............] - ETA: 0s - loss: 0.5411Epoch 14/100\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3584 - val_loss: 0.3899\n",
      "184/242 [=====================>........] - ETA: 0s - loss: 0.6667Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3731 - val_loss: 1.0932\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5199 - val_loss: 0.4951\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5310 - val_loss: 1.0420\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5577 - val_loss: 0.5169\n",
      " 42/242 [====>.........................] - ETA: 0s - loss: 0.3646Epoch 14/100\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3737 - val_loss: 0.3569\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.3028Epoch 14/100\n",
      " 54/242 [=====>........................] - ETA: 0s - loss: 0.532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3392 - val_loss: 0.7941\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3445 - val_loss: 0.3762\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5803 - val_loss: 0.5666\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6561 - val_loss: 0.6179\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6251 - val_loss: 0.6012\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3429 - val_loss: 0.3472\n",
      "213/242 [=========================>....] - ETA: 0s - loss: 0.3597Epoch 14/100\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.3589.] - ETA: 0s - loss: 0.663oss: 0.543\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3572 - val_loss: 0.3805\n",
      "120/242 [=============>................] - ETA: 0s - loss: 0.3435Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3704 - val_loss: 1.1227\n",
      "135/242 [===============>..............] - ETA: 0s - loss: 0.5722Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5099 - val_loss: 0.4851\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5473 - val_loss: 0.5074\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3710 - val_loss: 0.3669\n",
      "Epoch 15/100\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5200 - val_loss: 0.8778\n",
      "169/242 [===================>..........] - ETA: 0s - loss: 0.3383Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6420 - val_loss: 0.6226\n",
      " 75/242 [========>.....................] - ETA: 0s - loss: 0.5645Epoch 16/100\n",
      "113/242 [=============>................] - ETA: 0s - loss: 0.4827[CV] END ......learning_rate=0.005, n_hidden=3, n_neurons=75; total time=  16.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3395 - val_loss: 0.3384\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6138 - val_loss: 0.5951\n",
      " 96/242 [==========>...................] - ETA: 0s - loss: 0.5484Epoch 15/100\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5711 - val_loss: 0.5512\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3397 - val_loss: 0.3349\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3541 - val_loss: 0.3843=====>......] - ETA: 0s - loss: 0.598\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3669 - val_loss: 1.3096\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3685 - val_loss: 0.4425\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5371 - val_loss: 0.5050\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5006 - val_loss: 0.4751\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5099 - val_loss: 0.7269\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6289 - val_loss: 0.6631\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6031 - val_loss: 0.5840\n",
      " 65/242 [=======>......................] - ETA: 0s - loss: 0.5105Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5618 - val_loss: 0.5409\n",
      "Epoch 17/100\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3838\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3378 - val_loss: 0.3479\n",
      "141/242 [================>.............] - ETA: 0s - loss: 0.504...] - ETA: 0s - loss: 0.5462Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3363 - val_loss: 0.3494\n",
      "174/242 [====================>.........] - ETA: 0s - loss: 0.3633Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3523 - val_loss: 0.3820==>..........] - ETA: 0s - loss: 0.5\n",
      "230/242 [===========================>..] - ETA: 0s - loss: 0.5918Epoch 17/100\n",
      "175/242 [====================>.........] - ETA: 0s - loss: 0.3315[CV] END ......learning_rate=0.005, n_hidden=3, n_neurons=18; total time=  18.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3655 - val_loss: 0.3531\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5278 - val_loss: 0.4915\n",
      "Epoch 17/100\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5009 - val_loss: 0.6313\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4920 - val_loss: 0.4668\n",
      "102/242 [===========>..................] - ETA: 0s - loss: 0.3704Epoch 17/100\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6167 - val_loss: 0.7347\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5927 - val_loss: 0.5862\n",
      " 75/242 [========>.....................] - ETA: 0s - loss: 0.3552Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5533 - val_loss: 0.5328\n",
      " 78/242 [========>.....................] - ETA: 0s - loss: 0.3554Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3336 - val_loss: 0.3234\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3347 - val_loss: 0.3564=====>.........] - ETA: 0s - loss: 0.359\n",
      "177/242 [====================>.........] - ETA: 0s - loss: 0.3582Epoch 17/100\n",
      "242/242 [==============================] - 3s 5ms/step - loss: 1.2248 - val_loss: 2.3634\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3503 - val_loss: 0.3765ETA: 0s - loss: 0.517\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3636 - val_loss: 0.3724\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5833 - val_loss: 0.5605\n",
      "136/242 [===============>..............] - ETA: 0s - loss: 0.3490Epoch 18/100\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5186 - val_loss: 0.4850\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.7175Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6053 - val_loss: 0.8212\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4924 - val_loss: 0.5651\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4844 - val_loss: 0.4610\n",
      "Epoch 19/100\n",
      "Epoch 18/100\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5451 - val_loss: 0.5255\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3327 - val_loss: 0.3685\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3322 - val_loss: 0.3405\n",
      " 92/242 [==========>...................] - ETA: 0s - loss: 0.4879Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5156 - val_loss: 0.9556\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3476 - val_loss: 0.3756\n",
      "170/242 [====================>.........] - ETA: 0s - loss: 0.5906Epoch 19/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.9482 - val_loss: 0.6304\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5102 - val_loss: 0.4751\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5737 - val_loss: 0.5657\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5372 - val_loss: 0.5245\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5947 - val_loss: 0.9305\n",
      "Epoch 20/100\n",
      "Epoch 20/100\n",
      "Epoch 19/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.2559Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3583 - val_loss: 0.3635\n",
      " 52/242 [=====>........................] - ETA: 1s - loss: 0.4964Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4847 - val_loss: 0.5217\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4775 - val_loss: 0.4519\n",
      " 30/242 [==>...........................] - ETA: 0s - loss: 0.3412Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3292 - val_loss: 0.3411\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3301 - val_loss: 0.3541\n",
      "129/242 [==============>...............] - ETA: 0s - loss: 0.5362Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4230 - val_loss: 0.3972 0.498\n",
      "223/242 [==========================>...] - ETA: 0s - loss: 0.5042Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3464 - val_loss: 0.3582\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4859 - val_loss: 0.5306\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5650 - val_loss: 0.5450\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5021 - val_loss: 0.4685\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5846 - val_loss: 1.0681\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5301 - val_loss: 0.5080\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3560 - val_loss: 0.3981\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4776 - val_loss: 0.4874\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4708 - val_loss: 0.4466\n",
      " 66/242 [=======>......................] - ETA: 0s - loss: 0.4898Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3271 - val_loss: 0.3153....] - ETA: 0s - loss: 0.358\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3268 - val_loss: 0.3743\n",
      "140/242 [================>.............] - ETA: 0s - loss: 0.5350Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3436 - val_loss: 0.3738\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3890 - val_loss: 0.4142\n",
      "165/242 [===================>..........] - ETA: 0s - loss: 0.4685Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5566 - val_loss: 0.5451\n",
      "189/242 [======================>.......] - ETA: 0s - loss: 0.3208Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4239 - val_loss: 0.5764\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4946 - val_loss: 0.4640\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5232 - val_loss: 0.4999\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5754 - val_loss: 1.1954\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4712 - val_loss: 0.4623\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3543 - val_loss: 0.3683\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4649 - val_loss: 0.4453\n",
      "Epoch 21/100\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3242 - val_loss: 0.3393\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3255 - val_loss: 0.3678\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3723 - val_loss: 0.4140\n",
      " 54/242 [=====>........................] - ETA: 0s - loss: 0.3074Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3430 - val_loss: 0.3528\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5487 - val_loss: 0.5360\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3965 - val_loss: 0.3735\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5665 - val_loss: 1.3420\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4873 - val_loss: 0.4557\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5164 - val_loss: 0.4938\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4651 - val_loss: 0.4502\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3526 - val_loss: 0.3747\n",
      " 56/242 [=====>........................] - ETA: 0s - loss: 0.5659Epoch 22/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4592 - val_loss: 0.4346\n",
      "  1/242 [..............................] - ETA: 3s - loss: 0.6285Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3233 - val_loss: 0.3213 loss: 0.474\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3234 - val_loss: 0.3219\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3613 - val_loss: 0.3785\n",
      "218/242 [==========================>...] - ETA: 0s - loss: 0.5590Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3400 - val_loss: 0.3604\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5411 - val_loss: 0.5236\n",
      "237/242 [============================>.] - ETA: 0s - loss: 0.4796Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5582 - val_loss: 1.4573\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3810 - val_loss: 0.3754\n",
      "156/242 [==================>...........] - ETA: 0s - loss: 0.3459Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4803 - val_loss: 0.4457\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5101 - val_loss: 0.4873\n",
      "232/242 [===========================>..] - ETA: 0s - loss: 0.3231Epoch 23/100\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4597 - val_loss: 0.4372\n",
      "  1/242 [..............................] - ETA: 2s - loss: 0.2334Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3522 - val_loss: 0.3534\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4545 - val_loss: 0.4340\n",
      " 74/242 [========>.....................] - ETA: 0s - loss: 0.4907Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3233 - val_loss: 0.3139\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3207 - val_loss: 0.3064\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3531 - val_loss: 0.3336\n",
      "134/242 [===============>..............] - ETA: 0s - loss: 0.3595Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3387 - val_loss: 0.3921\n",
      "  8/242 [..............................] - ETA: 1s - loss: 0.4528Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5339 - val_loss: 0.5230\n",
      "137/242 [===============>..............] - ETA: 0s - loss: 0.3065Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5505 - val_loss: 1.6008\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3682 - val_loss: 0.4857\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5041 - val_loss: 0.4807\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4547 - val_loss: 0.4320\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4743 - val_loss: 0.4437\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.5401Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3508 - val_loss: 0.3559\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4495 - val_loss: 0.4276.......] - ETA: 0s - loss: 0.326\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3192 - val_loss: 0.3336\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3190 - val_loss: 0.3872\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3373 - val_loss: 0.3514\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3466 - val_loss: 0.3440\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5272 - val_loss: 0.5126\n",
      "188/242 [======================>.......] - ETA: 0s - loss: 0.4478Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3607 - val_loss: 0.6653\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4983 - val_loss: 0.4782\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3479 - val_loss: 0.4202\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4681 - val_loss: 0.4413\n",
      "Epoch 25/100\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4501 - val_loss: 0.4287\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.5821Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4449 - val_loss: 0.4210\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3174 - val_loss: 0.3324\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3175 - val_loss: 0.3227\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3360 - val_loss: 0.3639\n",
      "143/242 [================>.............] - ETA: 0s - loss: 0.4415Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3423 - val_loss: 0.3332\n",
      "164/242 [===================>..........] - ETA: 0s - loss: 0.4427Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5207 - val_loss: 0.5087\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4929 - val_loss: 0.4752\n",
      "147/242 [=================>............] - ETA: 0s - loss: 0.3313Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3568 - val_loss: 0.6771\n",
      "181/242 [=====================>........] - ETA: 0s - loss: 0.3202Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4458 - val_loss: 0.4271\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3512 - val_loss: 0.3776\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4625 - val_loss: 0.4361\n",
      "Epoch 26/100\n",
      "Epoch 26/100\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4416 - val_loss: 0.4215\n",
      " 14/242 [>.............................] - ETA: 0s - loss: 0.3193Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3164 - val_loss: 0.3627\n",
      " 91/242 [==========>...................] - ETA: 0s - loss: 0.3238Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3134 - val_loss: 0.3686\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5149 - val_loss: 0.4983\n",
      "199/242 [=======================>......] - ETA: 0s - loss: 0.3467Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3341 - val_loss: 0.3598===>........] - ETA: 0s - loss: 0.435\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3368 - val_loss: 0.4491\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4879 - val_loss: 0.4675\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4572 - val_loss: 0.4328\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3503 - val_loss: 0.8687\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3484 - val_loss: 0.3524\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4418 - val_loss: 0.4252\n",
      " 18/242 [=>............................] - ETA: 0s - loss: 0.3950Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4377 - val_loss: 0.4182\n",
      "177/242 [====================>.........] - ETA: 0s - loss: 0.4968Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3142 - val_loss: 0.4104\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3149 - val_loss: 0.3162\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3330 - val_loss: 0.3792\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3358 - val_loss: 0.3436\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5091 - val_loss: 0.4886\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.2559Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4830 - val_loss: 0.4628\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4521 - val_loss: 0.4259\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3448 - val_loss: 0.8619\n",
      "162/242 [===================>..........] - ETA: 0s - loss: 0.3171Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3473 - val_loss: 0.3754\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4381 - val_loss: 0.4235\n",
      "168/242 [===================>..........] - ETA: 0s - loss: 0.4930Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4340 - val_loss: 0.4146\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3122 - val_loss: 0.3253\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3133 - val_loss: 0.3102\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5036 - val_loss: 0.4863\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3316 - val_loss: 0.3813\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3331 - val_loss: 0.4062\n",
      " 41/242 [====>.........................] - ETA: 0s - loss: 0.4744Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4783 - val_loss: 0.45980\n",
      "132/242 [===============>..............] - ETA: 0s - loss: 0.4977Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4474 - val_loss: 0.4307\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4346 - val_loss: 0.4229 0.311\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3408 - val_loss: 1.0590\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3448 - val_loss: 0.3469\n",
      "237/242 [============================>.] - ETA: 0s - loss: 0.3116Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4309 - val_loss: 0.4141\n",
      " 42/242 [====>.........................] - ETA: 0s - loss: 0.5053Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3099 - val_loss: 0.3066\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3108 - val_loss: 0.3423\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3307 - val_loss: 0.3318\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4987 - val_loss: 0.4778\n",
      "Epoch 31/100\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3291 - val_loss: 0.3198\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4742 - val_loss: 0.4543\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4429 - val_loss: 0.4229\n",
      " 62/242 [======>.......................] - ETA: 0s - loss: 0.4707Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4313 - val_loss: 0.4189\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3388 - val_loss: 1.0880\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3427 - val_loss: 0.3337\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4280 - val_loss: 0.4122\n",
      "146/242 [=================>............] - ETA: 0s - loss: 0.3245Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3085 - val_loss: 0.3014\n",
      "Epoch 30/100\n",
      "147/242 [=================>............] - ETA: 0s - loss: 0.4504"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3095 - val_loss: 0.3494\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4939 - val_loss: 0.4724\n",
      "184/242 [=====================>........] - ETA: 0s - loss: 0.3495Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3295 - val_loss: 0.3644\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3239 - val_loss: 0.3440\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4700 - val_loss: 0.4479ETA: 0s - loss: 0.309\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4387 - val_loss: 0.4151\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3417 - val_loss: 0.3674\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4248 - val_loss: 0.4059\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3345 - val_loss: 0.8571\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4282 - val_loss: 0.4181========>....] - ETA: 0s - loss: 0.499\n",
      "147/242 [=================>............] - ETA: 0s - loss: 0.4545Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3072 - val_loss: 0.3061\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3080 - val_loss: 0.3087\n",
      "148/242 [=================>............] - ETA: 0s - loss: 0.4350Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3283 - val_loss: 0.3633\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4893 - val_loss: 0.4720\n",
      "Epoch 32/100\n",
      "139/242 [================>.............] - ETA: 0s - loss: 0.3228Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3224 - val_loss: 0.34104626\n",
      " 93/242 [==========>...................] - ETA: 0s - loss: 0.2996Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4661 - val_loss: 0.4436\n",
      "145/242 [================>.............] - ETA: 0s - loss: 0.4934Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4347 - val_loss: 0.4205\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3423 - val_loss: 0.3357\n",
      "114/242 [=============>................] - ETA: 0s - loss: 0.3257Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4223 - val_loss: 0.4170\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3297 - val_loss: 0.9124\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4254 - val_loss: 0.4136\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3060 - val_loss: 0.3114............] - ETA: 0s - loss: 0.339\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4852 - val_loss: 0.4705s: 0.404\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3053 - val_loss: 0.3004\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3271 - val_loss: 0.3541\n",
      "Epoch 33/100\n",
      "121/121 [==============================] - 0s 3ms/step ss: 0.341- loss: 0.3521\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3196 - val_loss: 0.3152\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4623 - val_loss: 0.4383\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3419 - val_loss: 0.3574\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4311 - val_loss: 0.4131\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4199 - val_loss: 0.4059\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4226 - val_loss: 0.4109\n",
      "Epoch 33/100\n",
      "211/242 [=========================>....] - ETA: 0s - loss: 0.3230[CV] END ......learning_rate=0.005, n_hidden=3, n_neurons=98; total time=  18.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3054 - val_loss: 0.2995\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4812 - val_loss: 0.4652\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3043 - val_loss: 0.3628\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3259 - val_loss: 0.3382\n",
      "149/242 [=================>............] - ETA: 0s - loss: 0.4260Epoch 33/100\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3171 - val_loss: 0.3444- loss: 0.414\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4587 - val_loss: 0.4365\n",
      "212/242 [=========================>....] - ETA: 0s - loss: 0.4183Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3403 - val_loss: 0.3326\n",
      "115/242 [=============>................] - ETA: 0s - loss: 0.3062Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4177 - val_loss: 0.4109\n",
      " 13/242 [>.............................] - ETA: 1s - loss: 0.2763Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4276 - val_loss: 0.4133\n",
      "130/242 [===============>..............] - ETA: 0s - loss: 0.3327Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4199 - val_loss: 0.4076\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4774 - val_loss: 0.4530\n",
      "Epoch 36/100\n",
      "138/242 [================>.............] - ETA: 0s - loss: 0.452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3033 - val_loss: 0.3272\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3243 - val_loss: 0.3434\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3038 - val_loss: 0.3097\n",
      "180/242 [=====================>........] - ETA: 0s - loss: 0.4371Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3144 - val_loss: 0.3309\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4553 - val_loss: 0.4333\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3377 - val_loss: 0.3462\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4155 - val_loss: 0.3976\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4243 - val_loss: 0.4094\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4175 - val_loss: 0.4022\n",
      "161/242 [==================>...........] - ETA: 0s - loss: 1.3888Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4738 - val_loss: 0.4503\n",
      " 90/242 [==========>...................] - ETA: 0s - loss: 0.4171Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3235 - val_loss: 0.3497\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3016 - val_loss: 0.3497\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3017 - val_loss: 0.3597\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3129 - val_loss: 0.3060\n",
      " 70/242 [=======>......................] - ETA: 0s - loss: 0.3058Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4521 - val_loss: 0.4302\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3375 - val_loss: 0.3286\n",
      " 65/242 [=======>......................] - ETA: 0s - loss: 0.4784Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4134 - val_loss: 0.4016\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 3s 7ms/step - loss: 1.1282 - val_loss: 2.1888\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4211 - val_loss: 0.4095........] - ETA: 0s - loss: 0.290\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4151 - val_loss: 0.3988\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4705 - val_loss: 0.4463............] - ETA: 0s - loss: 0.397\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3023 - val_loss: 0.3407\n",
      " 50/242 [=====>........................] - ETA: 0s - loss: 0.4752Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3008 - val_loss: 0.2979\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3232 - val_loss: 0.3324\n",
      "Epoch 37/100\n",
      "236/242 [============================>.] - ETA: 0s - loss: 0.4116Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4491 - val_loss: 0.4254\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3109 - val_loss: 0.36880s - loss: 0.458\n",
      "Epoch 21/100\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.5862\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3367 - val_loss: 0.3393\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4115 - val_loss: 0.3943\n",
      "146/242 [=================>............] - ETA: 0s - loss: 0.3064Epoch 37/100\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4183 - val_loss: 0.4041\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5433 - val_loss: 0.5990\n",
      "170/242 [====================>.........] - ETA: 0s - loss: 0.3036Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4128 - val_loss: 0.3946\n",
      "239/242 [============================>.] - ETA: 0s - loss: 0.3007Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4673 - val_loss: 0.4449\n",
      "Epoch 39/100\n",
      " 77/242 [========>.....................] - ETA: 0s - loss: 0.4178[CV] END .....learning_rate=0.0005, n_hidden=1, n_neurons=75; total time=  42.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3214 - val_loss: 0.3247\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3010 - val_loss: 0.3390\n",
      "198/242 [=======================>......] - ETA: 0s - loss: 0.4084Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3003 - val_loss: 0.3083\n",
      "Epoch 37/100\n",
      "152/242 [=================>............] - ETA: 0s - loss: 0.4226Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4461 - val_loss: 0.4226 0.372\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3090 - val_loss: 0.3630\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.3977\n",
      "144/242 [================>.............] - ETA: 0s - loss: 0.3098Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3360 - val_loss: 0.3448\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4404 - val_loss: 0.4027\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4105 - val_loss: 0.3935\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4153 - val_loss: 0.4108\n",
      "Epoch 38/100\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4644 - val_loss: 0.4390\n",
      "224/242 [==========================>...] - ETA: 0s - loss: 0.3008Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2986 - val_loss: 0.3354\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3200 - val_loss: 0.3266\n",
      "131/242 [===============>..............] - ETA: 0s - loss: 0.4569Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2977 - val_loss: 0.2952\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4434 - val_loss: 0.4207s - loss: 0.407\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3079 - val_loss: 0.3073\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4074 - val_loss: 0.4051\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3353 - val_loss: 0.3551\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.3886\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4092 - val_loss: 0.4495\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4128 - val_loss: 0.4118\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4616 - val_loss: 0.4348\n",
      "Epoch 41/100\n",
      "192/242 [======================>.......] - ETA: 0s - loss: 0.6284Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2969 - val_loss: 0.2982\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3186 - val_loss: 0.3229\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2971 - val_loss: 0.3337\n",
      "242/242 [==============================] - 1s 5ms/steposs: 0.407 - loss: 0.4409 - val_loss: 0.4169\n",
      " 57/242 [======>.......................] - ETA: 0s - loss: 0.3109Epoch 41/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3061 - val_loss: 0.3271\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4060 - val_loss: 0.3990\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 3s 6ms/step - loss: 0.6365 - val_loss: 1.2551\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3351 - val_loss: 0.3224\n",
      "207/242 [========================>.....] - ETA: 0s - loss: 0.2963Epoch 40/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4067 - val_loss: 0.3860\n",
      " 82/242 [=========>....................] - ETA: 0s - loss: 0.5600Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3950 - val_loss: 0.3911\n",
      "208/242 [========================>.....] - ETA: 0s - loss: 0.4350Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4589 - val_loss: 0.4337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/242 [==================>...........] - ETA: 0s - loss: 0.3032Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4101 - val_loss: 0.4008\n",
      " 95/242 [==========>...................] - ETA: 0s - loss: 0.5374Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2946 - val_loss: 0.3061\n",
      "237/242 [============================>.] - ETA: 0s - loss: 0.4013Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3183 - val_loss: 0.3109\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4384 - val_loss: 0.4146\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2959 - val_loss: 0.3442\n",
      "Epoch 42/100\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3046 - val_loss: 0.3203\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4043 - val_loss: 0.3950\n",
      "Epoch 25/100\n",
      " 56/242 [=====>........................] - ETA: 0s - loss: 0.4046Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4831 - val_loss: 5.5517\n",
      "213/242 [=========================>....] - ETA: 0s - loss: 0.4114Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3346 - val_loss: 0.3750..........] - ETA: 0s - loss: 0.319\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4048 - val_loss: 0.3837\n",
      " 26/242 [==>...........................] - ETA: 0s - loss: 0.3277Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3811 - val_loss: 0.3861\n",
      "117/242 [=============>................] - ETA: 0s - loss: 0.3178Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4564 - val_loss: 0.4328\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4078 - val_loss: 0.3969\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2937 - val_loss: 0.3618\n",
      "Epoch 41/100\n",
      "242/242 [=======================>......] - ETA: 0s - loss: 0.3297=========================] - 1s 5ms/step - loss: 0.3164 - val_loss: 0.3139\n",
      "174/242 [====================>.........] - ETA: 0s - loss: 0.4014Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2938 - val_loss: 0.3818\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4360 - val_loss: 0.4124\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3038 - val_loss: 0.3600\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5489 - val_loss: 13.8618\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4025 - val_loss: 0.3919\n",
      "109/242 [============>.................] - ETA: 0s - loss: 0.3120Epoch 4/100\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3332 - val_loss: 0.3381\n",
      "157/242 [==================>...........] - ETA: 0s - loss: 0.2997Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4030 - val_loss: 0.3821\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4055 - val_loss: 0.3983\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3697 - val_loss: 0.3761\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4540 - val_loss: 0.4270\n",
      "Epoch 42/100\n",
      "Epoch 44/100\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2954 - val_loss: 0.3163- loss: 0.317\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3148 - val_loss: 0.3486\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4338 - val_loss: 0.4100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2948 - val_loss: 0.2939\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "128/242 [==============>...............] - ETA: 0s - loss: 0.4487Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3020 - val_loss: 0.3250\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4010 - val_loss: 0.3851\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3327 - val_loss: 0.3297\n",
      " 39/242 [===>..........................] - ETA: 0s - loss: 0.4090Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4011 - val_loss: 0.3834\n",
      " 95/242 [==========>...................] - ETA: 0s - loss: nanEpoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4033 - val_loss: 0.3932\n",
      "217/242 [=========================>....] - ETA: 0s - loss: 0.4340Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3612 - val_loss: 0.3464\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2929 - val_loss: 0.3488.407\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4517 - val_loss: 0.4254\n",
      " 94/242 [==========>...................] - ETA: 0s - loss: 0.3852Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4318 - val_loss: 0.4078\n",
      " 23/242 [=>............................] - ETA: 1s - loss: 0.2553Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3143 - val_loss: 0.3077\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2925 - val_loss: 0.3288\n",
      " 26/242 [==>...........................] - ETA: 0s - loss: 0.4806Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3998 - val_loss: 0.3922\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/100\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3002 - val_loss: 0.3047\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3310 - val_loss: 0.3242\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3996 - val_loss: 0.3821: 0.451\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3535 - val_loss: 0.4912\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4011 - val_loss: 0.3988\n",
      " 10/242 [>.............................] - ETA: 1s - loss: 0.4197Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4496 - val_loss: 0.4226\n",
      "140/242 [================>.............] - ETA: 0s - loss: 0.4098Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2915 - val_loss: 0.3338\n",
      " 90/242 [==========>...................] - ETA: 0s - loss: 0.3863Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4297 - val_loss: 0.4059..] - ETA: 0s - loss: 0.378\n",
      "207/242 [========================>.....] - ETA: 0s - loss: 0.4032Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2913 - val_loss: 0.2872\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3132 - val_loss: 0.3151\n",
      "131/242 [===============>..............] - ETA: 0s - loss: 0.4010Epoch 44/100\n",
      "  3/242 [..............................] - ETA: 6s - loss: 0.4685Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan- ETA: 0s - loss: 0.398\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3982 - val_loss: 0.3909\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2983 - val_loss: 0.4067\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3319 - val_loss: 0.3504\n",
      " 61/242 [======>.......................] - ETA: 0s - loss: nanEpoch 29/100\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3979 - val_loss: 0.3825\n",
      "173/242 [====================>.........] - ETA: 0s - loss: 0.2985Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3992 - val_loss: 0.3930\n",
      "123/242 [==============>...............] - ETA: 0s - loss: 0.3905Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3507 - val_loss: 0.3325\n",
      "105/242 [============>.................] - ETA: 0s - loss: 0.3131Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4475 - val_loss: 0.4197\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2900 - val_loss: 0.2880\n",
      " 23/242 [=>............................] - ETA: 1s - loss: 0.4071Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4279 - val_loss: 0.4040.....] - ETA: 1s - loss: 0.43\n",
      " 38/242 [===>..........................] - ETA: 0s - loss: 0.3548Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3124 - val_loss: 0.3092\n",
      "144/242 [================>.............] - ETA: 0s - loss: 0.3968Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2911 - val_loss: 0.2931\n",
      "119/242 [=============>................] - ETA: 0s - loss: 0.4270Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "200/242 [=======================>......] - ETA: 0s - loss: 0.3971Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3968 - val_loss: 0.3826\n",
      "223/242 [==========================>...] - ETA: 0s - loss: 0.4408Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2973 - val_loss: 0.2987\n",
      "145/242 [================>.............] - ETA: 0s - loss: 0.2984Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3306 - val_loss: 0.3211\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3963 - val_loss: 0.3819\n",
      " 73/242 [========>.....................] - ETA: 0s - loss: 0.3156Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3972 - val_loss: 0.4022\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4456 - val_loss: 0.4184\n",
      "Epoch 46/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.6901Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2890 - val_loss: 0.3735\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4260 - val_loss: 0.4023- loss: 0.393\n",
      " 75/242 [========>.....................] - ETA: 0s - loss: 0.4375Epoch 48/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3468 - val_loss: 0.3798\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3111 - val_loss: 0.3437\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2897 - val_loss: 0.2919\n",
      "220/242 [==========================>...] - ETA: 0s - loss: 0.3220Epoch 12/100\n",
      "Epoch 47/100\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan...........] - ETA: 0s - loss: 0.256\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3957 - val_loss: 0.3792\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3285 - val_loss: 0.3604\n",
      "Epoch 47/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.2654Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2948 - val_loss: 0.2939\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3948 - val_loss: 0.3834\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4438 - val_loss: 0.416 loss: 0.2862\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4243 - val_loss: 0.4005\n",
      "124/242 [==============>...............] - ETA: 0s - loss: 0.4081Epoch 49/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2899 - val_loss: 0.4208\n",
      " 28/242 [==>...........................] - ETA: 0s - loss: 0.4538Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3423 - val_loss: 0.3418\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3108 - val_loss: 0.3341\n",
      "220/242 [==========================>...] - ETA: 0s - loss: 0.3358Epoch 48/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3954 - val_loss: 0.3898\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2878 - val_loss: 0.3447: 0.319\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3945 - val_loss: 0.3797\n",
      "139/242 [================>.............] - ETA: 0s - loss: 0.2972Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2936 - val_loss: 0.2996\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3315 - val_loss: 0.4395\n",
      "105/242 [============>.................] - ETA: 0s - loss: 0.4000Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3932 - val_loss: 0.3829\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4421 - val_loss: 0.4145\n",
      "107/242 [============>.................] - ETA: 0s - loss: 0.2850Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4227 - val_loss: 0.4002\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3936 - val_loss: 0.3894\n",
      "149/242 [=================>............] - ETA: 0s - loss: 0.2921Epoch 48/100\n",
      "116/242 [=============>................] - ETA: 0s - loss: 0.372"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3384 - val_loss: 0.3963\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2873 - val_loss: 0.2882\n",
      " 26/242 [==>...........................] - ETA: 0s - loss: 0.4012Epoch 48/100\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3103 - val_loss: 0.3352\n",
      " 59/242 [======>.......................] - ETA: 0s - loss: 0.4221Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2869 - val_loss: 0.2845\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan...........] - ETA: 0s - loss: 0.373\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3929 - val_loss: 0.3766\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2919 - val_loss: 0.3582\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3303 - val_loss: 0.3878\n",
      "185/242 [=====================>........] - ETA: 0s - loss: 0.3410Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3919 - val_loss: 0.3925\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.5434Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4403 - val_loss: 0.4128\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4211 - val_loss: 0.3983\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3918 - val_loss: 0.3856\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2858 - val_loss: 0.3075\n",
      " 54/242 [=====>........................] - ETA: 0s - loss: 0.4003Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3087 - val_loss: 0.3386\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3357 - val_loss: 0.4029\n",
      "187/242 [======================>.......] - ETA: 0s - loss: 0.4010Epoch 15/100\n",
      " 17/242 [=>............................] - ETA: 0s - loss: 0.2723Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2868 - val_loss: 0.3483\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan loss: 0.322\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3922 - val_loss: 0.3810\n",
      "106/242 [============>.................] - ETA: 0s - loss: 0.3128Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3278 - val_loss: 0.3218A: 0s - loss: 0.281\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2929 - val_loss: 0.3548\n",
      " 51/121 [===========>..................] - ETA: 0s - loss: nanEpoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3904 - val_loss: 0.3919\n",
      "240/242 [============================>.] - ETA: 0s - loss: 0.3904Epoch 50/100\n",
      "121/121 [==============================] - 0s 3ms/step - loss: nan\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4387 - val_loss: 0.4110\n",
      "232/242 [===========================>..] - ETA: 0s - loss: 0.3365Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4196 - val_loss: 0.3962\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3901 - val_loss: 0.3929\n",
      "109/242 [============>.................] - ETA: 0s - loss: 0.4402Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2852 - val_loss: 0.2858\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3077 - val_loss: 0.3159\n",
      "129/242 [==============>...............] - ETA: 0s - loss: 0.4361Epoch 50/100\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3342 - val_loss: 0.3240\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2853 - val_loss: 0.3444\n",
      "140/242 [================>.............] - ETA: 0s - loss: 0.4273Epoch 16/100\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3908 - val_loss: 0.3830\n",
      "156/242 [==================>...........] - ETA: 0s - loss: 0.4190Epoch 51/100\n",
      " 98/242 [===========>..................] - ETA: 0s - loss: 0.3002[CV] END .......learning_rate=0.05, n_hidden=3, n_neurons=74; total time=  16.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3264 - val_loss: 0.3178\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2912 - val_loss: 0.3203\n",
      "188/242 [======================>.......] - ETA: 0s - loss: 0.2792Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4371 - val_loss: 0.4095\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4183 - val_loss: 0.3962\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.3893 - val_loss: 0.3964\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3886 - val_loss: 0.4078\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2843 - val_loss: 0.4190\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3069 - val_loss: 0.3125\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3305 - val_loss: 0.3842\n",
      "101/242 [===========>..................] - ETA: 0s - loss: 0.4354Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2850 - val_loss: 0.2870..] - ETA: 0s - loss: 0.255\n",
      "126/242 [==============>...............] - ETA: 0s - loss: 0.4367Epoch 51/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3897 - val_loss: 0.3725\n",
      "146/242 [=================>............] - ETA: 0s - loss: 0.3902Epoch 52/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3256 - val_loss: 0.3338oss: 0.511\n",
      " 61/242 [======>.......................] - ETA: 0s - loss: 0.4443Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4356 - val_loss: 0.4078\n",
      "203/242 [========================>.....] - ETA: 0s - loss: 0.3053Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3880 - val_loss: 0.4044\n",
      "211/242 [=========================>....] - ETA: 0s - loss: 0.3033Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4168 - val_loss: 0.3951\n",
      "112/242 [============>.................] - ETA: 0s - loss: 0.3236Epoch 54/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.2888 - val_loss: 0.2913\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3871 - val_loss: 0.3877\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2859 - val_loss: 0.6254\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3068 - val_loss: 0.3341\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3285 - val_loss: 0.3962\n",
      "191/242 [======================>.......] - ETA: 0s - loss: 0.3859Epoch 18/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2836 - val_loss: 0.3185\n",
      "111/242 [============>.................] - ETA: 0s - loss: 0.3736Epoch 52/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3885 - val_loss: 0.3729\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3252 - val_loss: 0.3570\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4343 - val_loss: 0.4066\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3869 - val_loss: 0.4124\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2879 - val_loss: 0.3017 loss: 0.326\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4156 - val_loss: 0.3938\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3855 - val_loss: 0.3916\n",
      "204/242 [========================>.....] - ETA: 0s - loss: 0.3861Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2891 - val_loss: 0.7472\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3070 - val_loss: 0.3025\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 3s 8ms/step - loss: 0.6004 - val_loss: 0.5913\n",
      "236/242 [============================>.] - ETA: 0s - loss: 0.3255Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3258 - val_loss: 0.3210\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2824 - val_loss: 0.2998\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3876 - val_loss: 0.3858......] - ETA: 0s - loss: 0.403\n",
      " 47/242 [====>.........................] - ETA: 0s - loss: 0.2831Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3255 - val_loss: 0.3150\n",
      " 36/242 [===>..........................] - ETA: 0s - loss: 0.3859Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4328 - val_loss: 0.4052\n",
      "161/242 [==================>...........] - ETA: 0s - loss: 0.3136Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3856 - val_loss: 0.4215\n",
      " 52/242 [=====>........................] - ETA: 0s - loss: 0.3368Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2851 - val_loss: 0.3393\n",
      "192/242 [======================>.......] - ETA: 0s - loss: 0.3211Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4143 - val_loss: 0.3913\n",
      "177/242 [====================>.........] - ETA: 0s - loss: 0.3920Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3840 - val_loss: 0.3983\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2875 - val_loss: 0.3947\n",
      "Epoch 54/100\n",
      " 25/242 [==>...........................] - ETA: 0s - loss: 0.3683Epoch 54/100......] - ETA: 0s - loss: 0.318\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3047 - val_loss: 0.3102\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3965 - val_loss: 0.4662\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3252 - val_loss: 0.3357\n",
      "Epoch 20/100\n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2815 - val_loss: 0.2976\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3865 - val_loss: 0.3729\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4315 - val_loss: 0.4041\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3238 - val_loss: 0.3313\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3846 - val_loss: 0.4255\n",
      "147/242 [=================>............] - ETA: 0s - loss: 0.3720Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2858 - val_loss: 0.2991\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4131 - val_loss: 0.3899........] - ETA: 0s - loss: 0.395\n",
      "189/242 [======================>.......] - ETA: 0s - loss: 0.3228Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3827 - val_loss: 0.3982\n",
      "176/242 [====================>.........] - ETA: 0s - loss: 0.4454Epoch 55/100\n",
      "242/242 [============================..] - ETA: 0s - loss: 0.3266==] - 1s 5ms/step - loss: 0.2836 - val_loss: 0.3935\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3049 - val_loss: 0.3278\n",
      "Epoch 56/100\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3666 - val_loss: 0.9486\n",
      " 42/242 [====>.........................] - ETA: 0s - loss: 0.2950Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3204 - val_loss: 0.3910\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2808 - val_loss: 0.2863\n",
      "112/242 [============>.................] - ETA: 0s - loss: 0.2922Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3855 - val_loss: 0.3748\n",
      "187/242 [======================>.......] - ETA: 0s - loss: 0.4129Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4302 - val_loss: 0.4026\n",
      " 84/242 [=========>....................] - ETA: 0s - loss: 0.3477Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3228 - val_loss: 0.3419\n",
      "109/242 [============>.................] - ETA: 0s - loss: 0.3436Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3833 - val_loss: 0.4266- ETA: 0s - loss: 0.383\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4120 - val_loss: 0.3897\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2839 - val_loss: 0.3199\n",
      "Epoch 58/100\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3813 - val_loss: 0.3934\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2793 - val_loss: 0.3040\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3037 - val_loss: 0.3111\n",
      "211/242 [=========================>....] - ETA: 0s - loss: 0.4353Epoch 57/100\n",
      "Epoch 56/100\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3888...................] - ETA: 0s - loss: 0.274\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3188 - val_loss: 0.3328\n",
      " 63/242 [======>.......................] - ETA: 0s - loss: 0.3800Epoch 22/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3464 - val_loss: 1.1411\n",
      "204/242 [========================>.....] - ETA: 0s - loss: 0.2861Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3845 - val_loss: 0.3739\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4290 - val_loss: 0.4015\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3222 - val_loss: 0.3238\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.2796 - val_loss: 0.2913\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4108 - val_loss: 0.3874\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2831 - val_loss: 0.2898\n",
      "Epoch 41/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.1788[CV] END .....learning_rate=0.0005, n_hidden=2, n_neurons=88; total time= 1.1min\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3023 - val_loss: 0.3128\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3801 - val_loss: 0.3821\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2810 - val_loss: 0.3145\n",
      "Epoch 57/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.1847Epoch 57/100\n",
      "  1/242 [..............................] - ETA: 1s - "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3171 - val_loss: 0.4007\n",
      "182/242 [=====================>........] - ETA: 0s - loss: 0.2805Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3424 - val_loss: 0.3477\n",
      "150/242 [=================>............] - ETA: 0s - loss: 0.2838Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4279 - val_loss: 0.4003\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3834 - val_loss: 0.3865\n",
      " 67/242 [=======>......................] - ETA: 0s - loss: 0.3306Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3218 - val_loss: 0.3324\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2791 - val_loss: 0.2918\n",
      " 80/242 [========>.....................] - ETA: 0s - loss: 0.3670Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4098 - val_loss: 0.3892\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2825 - val_loss: 0.3594\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2795 - val_loss: 0.3107\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3788 - val_loss: 0.3789\n",
      " 14/242 [>.............................] - ETA: 0s - loss: 0.2637Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3026 - val_loss: 0.3499\n",
      "138/242 [================>.............] - ETA: 0s - loss: 0.2671Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3160 - val_loss: 0.3513\n",
      "194/242 [=======================>......] - ETA: 0s - loss: 0.4104Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3261 - val_loss: 0.3623\n",
      " 69/242 [=======>......................] - ETA: 0s - loss: 0.8819Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4267 - val_loss: 0.3996\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.3502Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3824 - val_loss: 0.3735\n",
      " 47/242 [====>.........................] - ETA: 0s - loss: 0.3352Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3201 - val_loss: 0.3139\n",
      " 85/242 [=========>....................] - ETA: 0s - loss: 0.3352Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4088 - val_loss: 0.3874\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2834 - val_loss: 0.3111\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.2787 - val_loss: 0.3220\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2782 - val_loss: 0.3150\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3776 - val_loss: 0.3821\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3023 - val_loss: 0.2968\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4255 - val_loss: 0.3990\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3149 - val_loss: 0.3461\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3208 - val_loss: 0.5011\n",
      "  2/242 [..............................] - ETA: 13s - loss: 0.3542Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3816 - val_loss: 0.3730\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.2956\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3205 - val_loss: 0.3449\n",
      "127/242 [==============>...............] - ETA: 0s - loss: 0.3700Epoch 60/100\n",
      "242/242 [==============================] - 3s 7ms/step - loss: 0.6554 - val_loss: 15.9935\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4078 - val_loss: 0.3882======>...] - ETA: 0s - loss: 0.378\n",
      "124/242 [==============>...............] - ETA: 0s - loss: 0.3198Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2812 - val_loss: 0.4709\n",
      "131/242 [===============>..............] - ETA: 0s - loss: 1.0202Epoch 44/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.2034[CV] END ......learning_rate=0.005, n_hidden=3, n_neurons=75; total time= 1.2min\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3764 - val_loss: 0.3771\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2775 - val_loss: 0.2843\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3008 - val_loss: 0.3157\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4244 - val_loss: 0.3981\n",
      " 13/242 [>.............................] - ETA: 2s - loss: 0.4555Epoch 63/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3116 - val_loss: 0.3465\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3808 - val_loss: 0.3707\n",
      " 60/242 [======>.......................] - ETA: 0s - loss: 0.4298Epoch 61/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3197 - val_loss: 0.3099\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3155 - val_loss: 0.3245\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7494 - val_loss: 4.4623\n",
      " 89/242 [==========>...................] - ETA: 0s - loss: 0.3019Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4068 - val_loss: 0.3854\n",
      "136/242 [===============>..............] - ETA: 0s - loss: 0.4235Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2810 - val_loss: 0.3792.............] - ETA: 0s - loss: 0.393\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3753 - val_loss: 0.3885=========>.] - ETA: 0s - loss: 0.422\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2771 - val_loss: 0.3368\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3003 - val_loss: 0.3032\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4234 - val_loss: 0.3964\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3096 - val_loss: 0.4037\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3801 - val_loss: 0.3761\n",
      "Epoch 62/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.3065Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3064 - val_loss: 0.3198\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3190 - val_loss: 0.3552\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4169 - val_loss: 0.4154\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4059 - val_loss: 0.3859\n",
      " 68/242 [=======>......................] - ETA: 0s - loss: 0.2934Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2795 - val_loss: 0.7198\n",
      "110/242 [============>.................] - ETA: 0s - loss: 0.3916Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3742 - val_loss: 0.3761\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2803 - val_loss: 0.2790\n",
      "Epoch 62/100\n",
      "242/242 [===============...............] - ETA: 0s - loss: 0.385===============] - 1s 5ms/step - loss: 0.4223 - val_loss: 0.3959\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3011 - val_loss: 0.3003\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3058 - val_loss: 0.3131\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3792 - val_loss: 0.3761\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3039 - val_loss: 0.5351\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3198 - val_loss: 0.3451........] - ETA: 0s - loss: 0.278\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4049 - val_loss: 0.3867\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4014 - val_loss: 0.3752\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 4s 7ms/step - loss: nan - val_loss: nan...] - ETA: 1s - loss: 0.282\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3730 - val_loss: 0.3721\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2835 - val_loss: 0.6889\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2752 - val_loss: 0.3209\n",
      "220/242 [==========================>...] - ETA: 0s - loss: 0.2947Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4214 - val_loss: 0.3946\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2993 - val_loss: 0.3039\n",
      " 84/242 [=========>....................] - ETA: 0s - loss: 0.3958Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3783 - val_loss: 0.3706\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2936 - val_loss: 0.3308\n",
      "155/242 [==================>...........] - ETA: 0s - loss: 0.2938Epoch 12/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3056 - val_loss: 0.4405\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3178 - val_loss: 0.3310 - loss: 0.271\n",
      "167/242 [===================>..........] - ETA: 0s - loss: 0.4208Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4041 - val_loss: 0.3815\n",
      " 47/242 [====>.........................] - ETA: 0s - loss: 0.3171Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3591 - val_loss: 0.4255\n",
      "214/242 [=========================>....] - ETA: 0s - loss: 0.2878Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan===========>.......] - ETA: 0s - loss: 0.294\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3720 - val_loss: 0.3690\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2847 - val_loss: 0.7176\n",
      " 92/242 [==========>...................] - ETA: 0s - loss: nanEpoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2752 - val_loss: 0.2857\n",
      " 40/242 [===>..........................] - ETA: 1s - loss: 0.3745Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4204 - val_loss: 0.3935\n",
      "227/242 [===========================>..] - ETA: 0s - loss: 0.2988Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2993 - val_loss: 0.2992\n",
      "135/242 [===============>..............] - ETA: 0s - loss: nanEpoch 65/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3775 - val_loss: 0.3746\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3038 - val_loss: 0.3014\n",
      "138/242 [================>.............] - ETA: 0s - loss: 0.2707Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3178 - val_loss: 0.3280\n",
      "158/242 [==================>...........] - ETA: 0s - loss: 0.2702Epoch 65/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.2937 - val_loss: 0.2723\n",
      "196/242 [=======================>......] - ETA: 0s - loss: 0.2678Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4033 - val_loss: 0.3814\n",
      " 69/242 [=======>......................] - ETA: 0s - loss: 0.3924Epoch 67/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3560 - val_loss: 0.3157\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      " 74/242 [========>.....................] - ETA: 0s - loss: 0.3383Epoch 4/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3710 - val_loss: 0.3702\n",
      "164/242 [===================>..........] - ETA: 0s - loss: 0.2931Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2803 - val_loss: 0.2980 0.336\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2742 - val_loss: 0.2874\n",
      "Epoch 65/100\n",
      "123/242 [==============>...............] - ETA: 0s - loss: 0.3526Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2979 - val_loss: 0.3244\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4195 - val_loss: 0.3927\n",
      " 78/242 [========>.....................] - ETA: 0s - loss: nanEpoch 68/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3767 - val_loss: 0.3688ETA: 0s - loss: 0.377\n",
      "143/242 [================>.............] - ETA: 0s - loss: 0.2668Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2873 - val_loss: 0.5143\n",
      " 27/242 [==>...........................] - ETA: 0s - loss: 0.3424Epoch 14/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3028 - val_loss: 0.3338\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3181 - val_loss: 0.3248\n",
      "224/242 [==========================>...] - ETA: 0s - loss: 0.2976Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4025 - val_loss: 0.3835\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3397 - val_loss: 0.3337\n",
      "  5/242 [..............................] - ETA: 6s - loss: 0.3828Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan - loss: 0.278\n",
      "104/242 [===========>..................] - ETA: 0s - loss: 0.2777Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3700 - val_loss: 0.3649\n",
      "126/242 [==============>...............] - ETA: 0s - loss: 0.2806Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2993 - val_loss: 0.2998...........] - ETA: 0s - loss: 0.286\n",
      " 47/242 [====>.........................] - ETA: 0s - loss: nanEpoch 67/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2760 - val_loss: 0.3133\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4184 - val_loss: 0.3919\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2774 - val_loss: 0.2883\n",
      "Epoch 66/100\n",
      "Epoch 69/100\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3758 - val_loss: 0.3800s - loss: 0.312.......] - ETA: 0s - loss: na\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3031 - val_loss: 0.2959\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2885 - val_loss: 0.5862\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3155 - val_loss: 0.3069\n",
      " 36/242 [===>..........................] - ETA: 0s - loss: 0.2741Epoch 67/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4017 - val_loss: 0.3816\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3354 - val_loss: 0.3470\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3690 - val_loss: 0.3696\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4176 - val_loss: 0.3909\n",
      "142/242 [================>.............] - ETA: 0s - loss: 0.3627Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2973 - val_loss: 0.2954\n",
      " 23/242 [=>............................] - ETA: 0s - loss: 0.4936Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2746 - val_loss: 0.3251\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2735 - val_loss: 0.3281\n",
      "168/242 [===================>..........] - ETA: 0s - loss: 0.2813Epoch 51/100\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3750 - val_loss: 0.3788======================>..] - ETA: 0s - loss: 0.278s - loss: 0.280\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2989 - val_loss: 0.3097\n",
      "239/242 [============================>.] - ETA: 0s - loss: nanEpoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2800 - val_loss: 0.3260\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3164 - val_loss: 0.3146\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4009 - val_loss: 0.3798\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3334 - val_loss: 0.3290\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4167 - val_loss: 0.3903\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3679 - val_loss: 0.3816\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2967 - val_loss: 0.3130\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2745 - val_loss: 0.3723\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2741 - val_loss: 0.2783\n",
      "230/242 [===========================>..] - ETA: 0s - loss: 0.3668Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3744 - val_loss: 0.3822 loss: 0.39\n",
      "148/242 [=================>............] - ETA: 0s - loss: 0.2686Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2961 - val_loss: 0.5226 ETA: 0s - loss: 0.26\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2823 - val_loss: 0.6647\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4002 - val_loss: 0.3794\n",
      " 15/242 [>.............................] - ETA: 0s - loss: 0.2741Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3143 - val_loss: 0.3085\n",
      " 46/242 [====>.........................] - ETA: 0s - loss: 0.3044Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3186 - val_loss: 0.3431\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4159 - val_loss: 0.3902\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3669 - val_loss: 0.3853\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2986 - val_loss: 0.3009\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2738 - val_loss: 0.2750\n",
      "178/242 [=====================>........] - ETA: 0s - loss: 0.2965Epoch 69/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2735 - val_loss: 0.2863\n",
      "199/242 [=======================>......] - ETA: 0s - loss: 0.3001Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3736 - val_loss: 0.3653............] - ETA: 0s - loss: 0.261\n",
      "137/242 [===============>..............] - ETA: 0s - loss: 0.2851Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2813 - val_loss: 0.4372\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2982 - val_loss: 0.4081\n",
      "205/242 [========================>.....] - ETA: 0s - loss: 0.2950Epoch 18/100\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3993 - val_loss: 0.3800\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3160 - val_loss: 0.3723 0.324\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4151 - val_loss: 0.3885\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3073 - val_loss: 2.0382\n",
      "Epoch 12/100\n",
      "220/242 [==========================>...] - ETA: 0s - loss: 0.2746Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3663 - val_loss: 0.3816\n",
      " 32/242 [==>...........................] - ETA: 0s - loss: 0.3027Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2973 - val_loss: 0.3068\n",
      "113/242 [=============>................] - ETA: 0s - loss: 0.3244Epoch 9/100\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2718 - val_loss: 0.2802\n",
      "164/242 [===================>..........] - ETA: 0s - loss: 0.2995Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2732 - val_loss: 0.3118- loss: 0.27\n",
      "206/242 [========================>.....] - ETA: 0s - loss: 0.2929Epoch 54/100\n",
      "182/242 [=====================>........] - ETA: 0s - loss: n.413"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter sets and ranges to explore\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [1, 2, 3, 4],\n",
    "    \"n_neurons\": list(range(10, 100)),\n",
    "    \"learning_rate\": [5e-4, 5e-3, 5e-2, 5e-1]    #default learning rate is 1e-2\n",
    "}\n",
    "                      \n",
    "# Create an instance of RandomizedSearchCV\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=20, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Search\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the parameters of the best model.\n",
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the score of the best model (note that Scikit-Learn computes a negative value)\n",
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model for the best estimator, and evaluate it on the test set.\n",
    "rnd_search_model = rnd_search_cv.best_estimator_.model\n",
    "rnd_search_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "There are many alternative techniques that can explore a search space more efficient than RandomSerarchCV. There are a list of libraries in the book on pages 322-333.\n",
    "\n",
    "I have tried BayesSearchCV, but results have been dissapointing so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Serving\n",
    "\n",
    "You can deploy a model as a REST API using TensorFlow Serving (TF Serving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you start TF Serving, you should export the model to TensorFlow's SavedModel format.\n",
    "model_version = \"02\"\n",
    "model_name = \"housing_model\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I will install TF Serving using a Docker image (there are also other options).\n",
    "\n",
    "First, you should pull the Docker image by typing the following command from the command prompt:\n",
    "\n",
    "docker pull tensorflow/serving\n",
    "\n",
    "Then you can run the docker image by typing a command similar to the following:\n",
    "\n",
    "docker run -it --rm -p 8500:8500 -p 8501:8501 -v \"/Users/hk/Documents/Undervisning/ML/Examples/housing_model:/models/housing_model\" -e MODEL_NAME=housing_model tensorflow/serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can make a prediction by querying TF Serving REST API. A query must be a POST request,\n",
    "# and the input data must be passed in the request body as a JSON object.\n",
    "\n",
    "(I will make the request in Postman)\n",
    "import json\n",
    "\n",
    "input_data_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist()\n",
    "})\n",
    "\n",
    "input_data_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
